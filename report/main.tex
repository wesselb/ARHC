%!TEX program = xelatex

\documentclass[letterpaper,11pt]{extarticle}
\input{preamble.tex}
\addbibresource{bibliography.bib}

\renewcommand{\DocTitle}{Coursework 2}
\renewcommand{\DocCourse}{MLSALT6}
\renewcommand{\DocAuthor}{James Requeima (jrr41), Wessel Bruinsma (wpb23)}

\usepackage{setspace}


\newcommand{\outputPath}{output/}

\begin{document}
\title
\singlespacing

\begingroup

In this document we briefly describe the design of the Adaptive Run-length Huffman Compressor (ARHC).

\section{Motivation}
% For the task of compressing $N=10,000$ bits, each having probability $p = 0.01$ of being a 1, the extra bit overhead of the Huffman algorithm prohibits naive application of it. Since the $0$ symbol is highly probable, its observations conveys virtually no information. On If we are observing the file bit-by-bit, the number of bits that pass before a one is observed does contain a lot of information. As an approach, we treat each of these zero-counts, or run-lengths, as a symbol for the Huffman algorithm. To make sure that our encoding and decoding algorithm is well defined, we pick a maximum run-length $n$ that defines the largest number of zeros denoted as a symbol. More precisely, let
For the task of compressing $N=10,000$ bits, each having probability $p = 0.01$ of being a 1, the extra bit overhead of the Huffman algorithm prohibits naive application of it; since the $0$ symbol is highly probable, its observation conveys virtually no information and so encoding it by a bit is wasteful.

Imagine that we observe the input file bit by bit. Then the numbers of bits that pass before ones are observed fully describe the file, and in addition all have high information content. This motivates the approach to define each of these zero-counts, also called run-lengths, as the symbols of our alphabet. However, to make sure that our approach is well defined, we pick a maximum run-length $n$ that defines the largest number of zeros denoted as a symbol. More precisely, let
\begin{align*}
    \mathcal{A}_X &= \big(1, 01, 001, \ldots, \underbrace{0\cdots0}_{n-1}\!1, \underbrace{0\cdots0}_{n} \big), \\
    \mathcal{P}_X &= \big( p, (1-p)p, \ldots, (1-p)^{n-1}p, (1-p)^n \big)
\end{align*}
be the ensemble that will be encoded by a Huffman code $C_X$.

It remains to determine an appropriate maximum run-length. Let $L$ denote the expected codelength and let $H$ denote the entropy of the distribution over the symbols in our symbol set. Then the expected number of wasted bits per symbol is  $L - H$. The red line in \Cref{fig:ratio} shows the ratio $L/H$ where $L$ is determined by explicitly computing the Huffman code for the associated $n$, and the dashed green line shows an approximation.\footnote{We can determine the optimal run-length observed in \cref{fig:ratio} by approximating the behaviour of the Huffman algorithm. First we exploit the structure of the symbols in $\mathcal{A}_X$ and decompose the entropy via
\begin{align*}
    H(X) = \underbrace{H_2(p) + p(H_2(p) + p(H_2(p) + \ldots))}_{n~\text{terms}}
    = H_2(p) \sum_{i=0}^{n-1} p = \frac{H_2(p)}{p}(1-(1-p)^n).
\end{align*}
Motivated by the fact that $1-p$ is small, we assume that the result of running the Huffman algorithm on the first $n$ symbols in $\mathcal{A}_X$ is similar to the result if they have equal probability. Now Exercise 5.28 in \cite{mackayinformation} yields that the first $\lfloor (n-1) f^- \rfloor$ symbols in $\mathcal{A}_X$ will have length $\lfloor \log_2 (n - 1) \rfloor + 1$ and the other  $n - 1 -\lceil (n-1) f^- \rceil$ symbols in $\mathcal{A}_X$ will have length $\lceil \log_2 (n - 1) \rceil + 1$ where $f^- = 2^{\lceil \log_2 (n - 1) \rceil}/(n-1) - 1$. We can then approximate the expected length by
\begin{align*}
    L \approx \sum_{i=1}^{\lfloor (n-1) f^- \rfloor} (1-p)^{i-1}p\cdot (\lfloor \log_2 (n - 1) \rfloor + 1) +
    \sum_{i=\lceil (n-1) f^- \rceil}^{n} (1-p)^{i-1}p\cdot (\lceil \log_2 (n - 1) \rceil + 1) + (1-p)^n.
\end{align*}
The approximation of $L/H$ using the above expression for $H$ and above approximation for $L$ is shown by the dashed green line in \cref{fig:ratio}. We see that the approximation holds perfectly for $n \le 100$ where inspection of the computed Huffman codes shows that the assumption indeed holds.} We see that this ratio attains extrema at multiples of $69$ and that $L$ closely approximates $H$ at these points.

The expected codeword length of the Huffman code is equal to the entropy when the distribution over the symbols in our symbol set is dyadic. Since all-zero symbol has probability $0.99^n$, in order to have $L/H$ close to one, we must have that $0.99^n \approx 2^{-k}$ for any natural $k$, which implies that $n \approx k\times 69$. We choose to use $n=69$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{\outputPath optimal_n}
    \caption{Ratio $L/H$ for different run-lengths $n$}
    \label{fig:ratio}
\end{figure}


\section{Description}
\Cref{alg:pseudo} provides a high-level overview of ARHC. Descriptions of the components of the program can be found in the source code or in the documentation in \code{documentation.epub}.

The implementation of the Huffman algorithm is facilitated through \code{Leaf} and \code{Node} objects that define a binary tree. The algorithm is initialised by encapsulating each symbol and its probability in a \code{Leaf} object, and the merge operation is performed by combining two \code{Leaf} objects into a \code{Node} object. A code word can be mapped to its symbol by providing it as an argument to the \code{decode} operation of the root \code{Node} of the tree. Additionally, the encoding of all code words can be obtained by calling the \code{getEncoding} operation of the root \code{Node} of the tree.

If $p$ is unknown, then the user can provide parameters $\alpha_0$, $\alpha_1$ for a prior $\operatorname{Beta}(\alpha_0, \alpha_1)$ over $p$. In lines 17 to 19 of \cref{alg:pseudo}, the posterior over $p$ is updated according to the observation $S$.

The operation of line 6 of \cref{alg:pseudo} dynamically adjusts the run-length so that
\begin{enumerate}
    \item the run-length is near optimal for the current estimate of $p$ and
    \item the symbols in $\mathcal{A}_X$ are able to entirely encode and decode the input stream\footnote{When $n>N$, without adjustment $\mathcal{A}_X$ might not be able to encode the remaining bits in the input stream, and some symbols in $\mathcal{A}_X$ will never be used.} without the use of an end-of-transmission symbol or the length of the transmitted message.
\end{enumerate}

Note that ARHC is able to perform partial compression and partial decompression; the compressor writes a symbol directly after the associated input bits are read and similarly the decompressor writes the input bits directly after the associated symbol is read. We will experiment with this behaviour by feeding the input bit by bit in \cref{sec:analysis}.

\begin{algorithm}[ht]
    \begin{algorithmic}[1]
        \Function{arhc}{$I$, $O$, $N$, $p$ or ($\alpha_0$ and $\alpha_1$)} \Comment{$I$ represents the input stream and $O$ represents the output stream}
            \If{adaptation}
                \State $p \gets \alpha_1 / (\alpha_0 + \alpha_1)$
            \EndIf
            \While{$N > 0$}
                \State $n \gets \max\{1,\min\{\operatorname{round}(-1/\log_2 (1 - p)) , N\}\}$
                \State Obtain the code $C_X$ by running the Huffman algorithm on the ensemble \begin{align*}
                    \mathcal{A}_X &= \big(1, 01, 001, \ldots, \underbrace{0\cdots0}_{n-1}\!1, \underbrace{0\cdots0}_{n} \big), \\
                    \mathcal{P}_X &= \big( p, (1-p)p, \ldots, (1-p)^{n-1}p, (1-p)^n \big)
                \end{align*}
                \If{compression}
                    \State $S \gets$ Read from $I$ the first symbol that corresponds with a symbol in $\mathcal{A}_X$
                    \State Write to $O$ the code word of $S$ according to $C_X$
                \ElsIf{decompression}
                    \State $S \gets$ Read from $I$ the first symbol that corresponds with a code word in $C_X$
                    \State Write to $O$ the symbol of $S$ in $\mathcal{A}_X$ according to $C_X$
                \EndIf
                \State $N \gets N - \operatorname{length}(S)$
                \If{adaptation}
                    \State $\alpha_0 \gets \alpha_0 + \operatorname{count}_0(S)$
                    \State $\alpha_1 \gets \alpha_1 + \operatorname{count}_1(S)$
                    \State $p \gets \alpha_1 / (\alpha_0 + \alpha_1)$
                \EndIf
            \EndWhile
        \EndFunction
    \end{algorithmic}
    \caption{Adaptive Run-length Huffman Compressor}
    \label{alg:pseudo}
\end{algorithm}

\section{Analysis}
\label{sec:analysis}
The source coding theorem tells us that we cannot compress the $N$-bits input to fewer than $NH_2(p)\approx807.9$ bits. We can approximate the length of the compressed input by $\hat{N}=NL/L_i$ where $L_i$ represents the expected number of inputs bits a symbol in $\mathcal{A}_X$ represents. We calculate $L_i$ via
\begin{align*}
    L_i = \sum_{i=1}^{n-1}(1-p)^{i-1}p\cdot i +(1-p)^n\cdot n
\end{align*}
which yields for $n=69$ that $\hat{N}=810.5$ bits; by averaging over 500 runs we indeed found that $\hat{N}\approx 812.3$. \Cref{tab:compression_performance} shows an overview of the compression performance of ARHC.

\begin{table}[ht]
    \centering
    \begin{tabular}{l|cc}
        \toprule
        & Expected length of the compressed input & Associated compression ratio \\ \midrule
        Optimal & $807.9$ bits & $0.9192$ \\
        ARHC & $810.5$ bits & $0.9190$ \\ \bottomrule
    \end{tabular}
    \caption{Compression performance of ARHC}
    \label{tab:compression_performance}
\end{table}

\Cref{fig:different_N} shows the compression performance of the static and adaptive compression scheme for different inputs lengths $N$. We see that for small $N$, the static compression scheme attains better compression ratio than the adaptive compression scheme, but also has higher variance. For large $N$, the static compression scheme and the adaptive compression scheme seem to perform similarly.

As the input is fed bit by bit to the compressor\footnote{Feeding the input bit by bit to the compressor and decompressor turned out to be difficult; it required us to flag the input and output streams as non-blocking and take into account the processing time of the compressor and decompressor.}, \Cref{tab:bits_highlight} shows the immediate output, i.e. partial compression and decompression output, of the compressor and decompressor. This allows us to inspect the dynamics of the adaptive compression scheme. We see that as the adaptive compressor observes more bits, it learns to efficiently encode the all-zero symbol.

\Cref{fig:comparison_mismatch} shows the compression performance in the case where the parameter $p$ of the static compression scheme matches the probability of the bent coin and the case where it does not. We see that in the former case the static compression scheme converges more quickly to the optimal compression ratio than the adaptive compression scheme, though with higher variance, but that in the latter case the adaptive compression scheme is able to adapt to the mismatched probability and eventually outperform the static compression scheme.

Finally, \cref{fig:comparison_priors} shows the effect of different settings of $\alpha_0$ and $\alpha_1$ on the compression performance of the adaptive compression scheme. We see that for small $\alpha_0$ and $\alpha_1$ the compressor is able to adapt more quickly, but also has higher variance. Furthermore, we observe that $\alpha_0=1.0$, $\alpha_1=1.0$ initially attains a better compression ratio. This shows that an appropriate prior can indeed help compression performance when little is known about the input.



\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{\outputPath Ns.pdf}
    \caption{Compression performance of the static compression scheme and the adaptive compression scheme for different input lengths $N$}
    \label{fig:different_N}
\end{figure}


\begin{table}[ht]
    \centering
    \vspace{-0.55cm}
    \setstretch{0.55}
    \centerline{\hspace{.5cm}\begin{subfigure}{.6\linewidth}
        \begin{tabular}{lp{3cm}p{4cm}}
            \toprule
            Input bit & Compressor output & Decompressor output \\ \midrule
            \expandableInput{\outputPath tab_static.tex}
            \bottomrule
        \end{tabular}
        \hspace{1cm}\caption{Static compression scheme}
    \end{subfigure}
    \begin{subfigure}{.6\linewidth}
        \begin{tabular}{lp{3cm}p{4cm}}
            \toprule
            Input bit & Compressor output & Decompressor output \\ \midrule
            \expandableInput{\outputPath tab_adaptive.tex}
            \bottomrule
        \end{tabular}
        \caption{Adaptive compression scheme}
    \end{subfigure}}
    \caption{Output of the compressor and decompressor when the input is fed bit by bit. The input consists of the first 80 bits of the bent-coin benchmark file \code{filep.01.10000NR}. The parameters of the prior are $\alpha_0=0.2$ and $\alpha_1=0.2$. The static compression scheme compresses down to 22 bits, the adaptive compression scheme compresses down to 23 bits and the SCT tells us that we cannot compress further than $80 H_2(0.01) \approx 6.5$ bits.}
    \label{tab:bits_highlight}
\end{table}


\begin{figure}[ht]
    \centering
    \begin{subfigure}{.49\linewidth}
        \includegraphics[width=\linewidth]{\outputPath comparison1}
        \caption{Compressor matches reality; the parameter of the static compressor scheme is $p=0.1$ and the bent coin has probability $p=0.1$}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \includegraphics[width=\linewidth]{\outputPath comparison1_mismatch}
        \caption{Compressor does not match reality; the parameter of the static compressor scheme is $p=0.1$ and the bent coin has probability $p=0.01$}
    \end{subfigure}
    \caption{Comparison of the static compression scheme and the adaptive compression scheme when a random 200-bit input is fed bit by bit. The parameters of the prior are $\alpha_0=0.1$ and $\alpha_1=0.1$. The lines and associated areas show respectively the means and 95\% confidence regions determined over 200 runs.}
    \label{fig:comparison_mismatch}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=.95\linewidth]{\outputPath comparison_priors}
    \caption{Compression performance of the adaptive compression scheme for different $\alpha_0$ and $\alpha_1$ when a random 200-bit input is fed bit by bit. The lines and associated areas show respectively the means and 95\% confidence regions determined over 50 runs.}
    \label{fig:comparison_priors}
\end{figure}

\endgroup
\printbibliography
%\bibliographystyle{plain}
%\bibliography{bibliography}

\end{document}
